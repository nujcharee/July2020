{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explore dataset\n",
    "- how many overall, how many blank and how many not helpful\n",
    "- Not helpful are for example nothing, no, none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('hospital.xls', skiprows=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['Org Name', 'Liked', 'Disliked']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "liked = data[['Liked']].dropna(subset=['Liked'])\n",
    "disliked = data[['Disliked']].dropna(subset=['Disliked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the first 1000 rows\n",
    "\n",
    "LikedComment= liked.loc[1:30,]\n",
    "DislikedComment = disliked.loc[1:30,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1           staff exceptionally friendly made feel ease\n",
       "2     professional friendly approach staff couldnt a...\n",
       "3     friendly staff times put ease arrival nothing ...\n",
       "4     care consideration received medical house staf...\n",
       "5     everyone reception staffpreop nursescleaners c...\n",
       "6     going hospital daunting experience stay agph r...\n",
       "7     friendly staff efficient sevice relaxed atmosp...\n",
       "8     beautiful surroundings hospital just world fou...\n",
       "9     enjoyed staythe staff wonderfulthe food good t...\n",
       "10    attended treatment centre surgery found hospit...\n",
       "11    liked fact every one friendly got hello thank ...\n",
       "12    never stay hospital really nervous apprehensiv...\n",
       "13    wonderfull hospital facilities exellent staff ...\n",
       "14    filling forms handing back nurse reception tea...\n",
       "15    moment walked door made feel ease helps dont k...\n",
       "16    total knee replacementthe treatment received e...\n",
       "17    admitted hospital nervous first operation ever...\n",
       "18    friendliness caring attitude professionalism t...\n",
       "19    expert professional treatment care first prior...\n",
       "20    husband daughter need surgery hospital last we...\n",
       "21    praise thank staff enough making weeks stay co...\n",
       "22    son treated nhs abbey gisburne park although d...\n",
       "23           food lovelymy consultant nice guy thorough\n",
       "24     hospital fantastic location enviable views sides\n",
       "25    initial appointments surgery hospital service ...\n",
       "26    individual rooms en suite bathroom direct phon...\n",
       "27    back operation spent night staff surgeon excep...\n",
       "28    exceptional care needed pain killers op bed bo...\n",
       "29    given first class care whilst inpatient contin...\n",
       "30    nothing father neglected time sent home withou...\n",
       "Name: Liked, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = get_stop_words('en')\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    x = re.sub('[^a-z\\s]', '', x.lower())                  \n",
    "    x = [w for w in x.split() if w not in set(stop_words)]  \n",
    "    return ' '.join(x)   \n",
    "\n",
    "LikedComment.Liked.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Liked    Very professional but with a friendly approach...\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liked.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Going into hospital is a daunting experience but my stay at AGPH was remarkable.  The doctors and nurses were extremely kind, patient and caring.  Nothing was too much trouble, even in the middle of the night.  What stood out to me was the way they all went about their duties with a caring and kind attitude and a smile on their faces.  The catering staff were very kind and again nothing was too much trouble.  AGPH is set in beautiful surroundings in the Ribble Valley and as soon as you enter the building there is a sense of peace and calm.  I was quite nervous when I first went in but my fears were soon allayed.  I would have no hesitation in recommending this hospital and if I ever need surgery again it would be my first choice.\"\n",
    "doc = nlp(text)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "print([token.text for token in doc])\n",
    "\n",
    "displacy.serve(doc, style=\"ent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match sequences of tokens, based on pattern rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"LOWER\": \"friendly\"}, {\"LOWER\": \"staff\"}]\n",
    "matcher.add(\"Staff\", None, pattern)\n",
    "\n",
    "# doc = nlp(\"Hello, world! Hello world!\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match sequences of tokens, based on documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"Staff\", None, nlp(\"Friendly Staff\"))\n",
    "#doc = nlp(\"Barack Obama lifts America one last time in emotional farewell\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(job_descriptions.data, job_descriptions.target, test_size=0.3, random_state=0)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "i = 0 \n",
    "for index, row in liked.iterrows():\n",
    "    \n",
    "        f = open(str(i)+'.txt', 'w')\n",
    "        f.write(row[0])\n",
    "        f.close()\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 \n",
    "for index, row in disliked.iterrows():\n",
    "    \n",
    "        f = open(str(i)+'.txt', 'w')\n",
    "        f.write(row[0])\n",
    "        f.close()\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = load_files('data/validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(comments.data, comments.target, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Register the Token extension attribute \"is_country\" with the default value False\n",
    "Token.set_extension(\"is_country\", default=False)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
